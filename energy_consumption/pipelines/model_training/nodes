"""
This module contains the model training nodes for the energy consumption pipeline.
"""
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, GRU, Dense, Dropout, Bidirectional
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.optimizers import Adam
import optuna
from optuna.integration import TFKerasPruningCallback
from typing import Dict, Any, Tuple, List
import logging
import statsmodels.api as sm
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.statespace.sarimax import SARIMAX
from prophet import Prophet
import xgboost as xgb
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ARIMA Models
def train_arima_models(arima_data: Dict[str, Any], params: Dict[str, Any]) -> Dict[str, Any]:
    """
    Train ARIMA models for each frequency.
    
    Args:
        arima_data: Dictionary containing data for ARIMA models
        params: Dictionary containing parameters
        
    Returns:
        Dictionary containing trained ARIMA models and predictions
    """
    logger.info("Training ARIMA models")
    
    result = {}
    
    for freq, data in arima_data.items():
        logger.info(f"Training ARIMA model for {freq}")
        
        train = data["train"]
        test = data["test"]
        
        # Get ARIMA parameters
        p = params.get("arima_p", 5)
        d = params.get("arima_d", 1)
        q = params.get("arima_q", 1)
        
        try:
            # Train ARIMA model
            model = ARIMA(train, order=(p, d, q))
            model_fit = model.fit()
            
            # Make predictions
            predictions = model_fit.forecast(steps=len(test))
            
            # Calculate metrics
            mse = mean_squared_error(test, predictions)
            rmse = np.sqrt(mse)
            mae = mean_absolute_error(test, predictions)
            r2 = r2_score(test, predictions)
            
            logger.info(f"ARIMA {freq} - MSE: {mse:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f}, R²: {r2:.4f}")
            
            result[freq] = {
                "model": model_fit,
                "predictions": predictions,
                "actual": test,
                "metrics": {
                    "mse": mse,
                    "rmse": rmse,
                    "mae": mae,
                    "r2": r2
                }
            }
        except Exception as e:
            logger.error(f"Error training ARIMA model for {freq}: {str(e)}")
            result[freq] = {
                "model": None,
                "predictions": None,
                "actual": test,
                "metrics": {
                    "mse": float('inf'),
                    "rmse": float('inf'),
                    "mae": float('inf'),
                    "r2": float('-inf')
                },
                "error": str(e)
            }
    
    return result

# SARIMA Models
def train_sarima_models(arima_data: Dict[str, Any], params: Dict[str, Any]) -> Dict[str, Any]:
    """
    Train SARIMA models for each frequency.
    
    Args:
        arima_data: Dictionary containing data for SARIMA models
        params: Dictionary containing parameters
        
    Returns:
        Dictionary containing trained SARIMA models and predictions
    """
    logger.info("Training SARIMA models")
    
    result = {}
    
    for freq, data in arima_data.items():
        logger.info(f"Training SARIMA model for {freq}")
        
        train = data["train"]
        test = data["test"]
        
        # Get SARIMA parameters
        p = params.get("sarima_p", 2)
        d = params.get("sarima_d", 1)
        q = params.get("sarima_q", 1)
        P = params.get("sarima_P", 1)
        D = params.get("sarima_D", 1)
        Q = params.get("sarima_Q", 1)
        
        # Set seasonal period based on frequency
        if freq == "hourly_data":
            s = 24  # Daily seasonality
        elif freq == "daily_data":
            s = 7   # Weekly seasonality
        else:  # weekly_data
            s = 52  # Yearly seasonality
        
        try:
            # Train SARIMA model
            model = SARIMAX(train, order=(p, d, q), seasonal_order=(P, D, Q, s))
            model_fit = model.fit(disp=False)
            
            # Make predictions
            predictions = model_fit.forecast(steps=len(test))
            
            # Calculate metrics
            mse = mean_squared_error(test, predictions)
            rmse = np.sqrt(mse)
            mae = mean_absolute_error(test, predictions)
            r2 = r2_score(test, predictions)
            
            logger.info(f"SARIMA {freq} - MSE: {mse:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f}, R²: {r2:.4f}")
            
            result[freq] = {
                "model": model_fit,
                "predictions": predictions,
                "actual": test,
                "metrics": {
                    "mse": mse,
                    "rmse": rmse,
                    "mae": mae,
                    "r2": r2
                }
            }
        except Exception as e:
            logger.error(f"Error training SARIMA model for {freq}: {str(e)}")
            result[freq] = {
                "model": None,
                "predictions": None,
                "actual": test,
                "metrics": {
                    "mse": float('inf'),
                    "rmse": float('inf'),
                    "mae": float('inf'),
                    "r2": float('-inf')
                },
                "error": str(e)
            }
    
    return result

# Prophet Models
def train_prophet_models(prophet_data: Dict[str, Any], params: Dict[str, Any]) -> Dict[str, Any]:
    """
    Train Prophet models for each frequency.
    
    Args:
        prophet_data: Dictionary containing data for Prophet models
        params: Dictionary containing parameters
        
    Returns:
        Dictionary containing trained Prophet models and predictions
    """
    logger.info("Training Prophet models")
    
    result = {}
    
    for freq, data in prophet_data.items():
        logger.info(f"Training Prophet model for {freq}")
        
        train_df = data["train"]
        test_df = data["test"]
        
        try:
            # Create and train Prophet model
            model = Prophet(
                yearly_seasonality=params.get("prophet_yearly_seasonality", True),
                weekly_seasonality=params.get("prophet_weekly_seasonality", True),
                daily_seasonality=params.get("prophet_daily_seasonality", True if freq == "hourly_data" else False),
                seasonality_mode=params.get("prophet_seasonality_mode", "additive"),
                changepoint_prior_scale=params.get("prophet_changepoint_prior_scale", 0.05)
            )
            
            # Add country holidays if specified
            if params.get("prophet_add_holidays", False):
                model.add_country_holidays(country_name='France')
            
            # Fit the model
            model.fit(train_df)
            
            # Create future dataframe for prediction
            future = model.make_future_dataframe(periods=len(test_df), freq='H' if freq == "hourly_data" else 'D' if freq == "daily_data" else 'W')
            
            # Make predictions
            forecast = model.predict(future)
            
            # Extract predictions for test period
            predictions = forecast.iloc[-len(test_df):]['yhat'].values
            
            # Calculate metrics
            mse = mean_squared_error(test_df['y'], predictions)
            rmse = np.sqrt(mse)
            mae = mean_absolute_error(test_df['y'], predictions)
            r2 = r2_score(test_df['y'], predictions)
            
            logger.info(f"Prophet {freq} - MSE: {mse:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f}, R²: {r2:.4f}")
            
            result[freq] = {
                "model": model,
                "forecast": forecast,
                "predictions": predictions,
                "actual": test_df['y'].values,
                "metrics": {
                    "mse": mse,
                    "rmse": rmse,
                    "mae": mae,
                    "r2": r2
                }
            }
        except Exception as e:
            logger.error(f"Error training Prophet model for {freq}: {str(e)}")
            result[freq] = {
                "model": None,
                "predictions": None,
                "actual": test_df['y'].values,
                "metrics": {
                    "mse": float('inf'),
                    "rmse": float('inf'),
                    "mae": float('inf'),
                    "r2": float('-inf')
                },
                "error": str(e)
            }
    
    return result

# XGBoost Models
def train_xgboost_models(ml_data: Dict[str, Any], params: Dict[str, Any]) -> Dict[str, Any]:
    """
    Train XGBoost models for each frequency.
    
    Args:
        ml_data: Dictionary containing data for ML models
        params: Dictionary containing parameters
        
    Returns:
        Dictionary containing trained XGBoost models and predictions
    """
    logger.info("Training XGBoost models")
    
    result = {}
    
    for freq, data in ml_data.items():
        logger.info(f"Training XGBoost model for {freq}")
        
        X_train = data["X_train"]
        y_train = data["y_train"]
        X_test = data["X_test"]
        y_test = data["y_test"]
        
        try:
            # Create and train XGBoost model
            model = xgb.XGBRegressor(
                n_estimators=params.get("xgb_n_estimators", 100),
                max_depth=params.get("xgb_max_depth", 6),
                learning_rate=params.get("xgb_learning_rate", 0.1),
                subsample=params.get("xgb_subsample", 0.8),
                colsample_bytree=params.get("xgb_colsample_bytree", 0.8),
                objective='reg:squarederror',
                random_state=42
            )
            
            # Fit the model
            model.fit(X_train, y_train)
            
            # Make predictions
            predictions = model.predict(X_test)
            
            # Calculate metrics
            mse = mean_squared_error(y_test, predictions)
            rmse = np.sqrt(mse)
            mae = mean_absolute_error(y_test, predictions)
            r2 = r2_score(y_test, predictions)
            
            # Get feature importances
            feature_importances = dict(zip(X_train.columns, model.feature_importances_))
            
            logger.info(f"XGBoost {freq} - MSE: {mse:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f}, R²: {r2:.4f}")
            
            result[freq] = {
                "model": model,
                "predictions": predictions,
                "actual": y_test,
                "feature_importances": feature_importances,
                "metrics": {
                    "mse": mse,
                    "rmse": rmse,
                    "mae": mae,
                    "r2": r2
                }
            }
        except Exception as e:
            logger.error(f"Error training XGBoost model for {freq}: {str(e)}")
            result[freq] = {
                "model": None,
                "predictions": None,
                "actual": y_test,
                "metrics": {
                    "mse": float('inf'),
                    "rmse": float('inf'),
                    "mae": float('inf'),
                    "r2": float('-inf')
                },
                "error": str(e)
            }
    
    return result

# RNN Models with Optuna optimization
def create_rnn_model(trial: optuna.Trial, input_shape: Tuple[int, int]) -> tf.keras.Model:
    """
    Create an RNN model with hyperparameters suggested by Optuna.
    
    Args:
        trial: Optuna trial object
        input_shape: Shape of the input data
        
    Returns:
        Compiled Keras model
    """
    # Define hyperparameters to optimize
    rnn_type = trial.suggest_categorical("rnn_type", ["LSTM", "GRU"])
    n_layers = trial.suggest_int("n_layers", 1, 3)
    n_units = trial.suggest_int("n_units", 32, 256, log=True)
    dropout_rate = trial.suggest_float("dropout_rate", 0.0, 0.5)
    learning_rate = trial.suggest_float("learning_rate", 1e-5, 1e-2, log=True)
    bidirectional = trial.suggest_categorical("bidirectional", [True, False])
    
    # Create model
    model = Sequential()
    
    # Add RNN layers
    for i in range(n_layers):
        return_sequences = i < n_layers - 1
        
        if i == 0:
            # First layer
            if rnn_type == "LSTM":
                if bidirectional:
                    model.add(Bidirectional(LSTM(n_units, return_sequences=return_sequences), 
                                           input_shape=input_shape))
                else:
                    model.add(LSTM(n_units, return_sequences=return_sequences, 
                                  input_shape=input_shape))
            else:  # GRU
                if bidirectional:
                    model.add(Bidirectional(GRU(n_units, return_sequences=return_sequences), 
                                           input_shape=input_shape))
                else:
                    model.add(GRU(n_units, return_sequences=return_sequences, 
                                 input_shape=input_shape))
        else:
            # Subsequent layers
            if rnn_type == "LSTM":
                if bidirectional:
                    model.add(Bidirectional(LSTM(n_units, return_sequences=return_sequences)))
                else:
                    model.add(LSTM(n_units, return_sequences=return_sequences))
            else:  # GRU
                if bidirectional:
                    model.add(Bidirectional(GRU(n_units, return_sequences=return_sequences)))
                else:
                    model.add(GRU(n_units, return_sequences=return_sequences))
        
        # Add dropout after each RNN layer
        model.add(Dropout(dropout_rate))
    
    # Output layer
    model.add(Dense(1))
    
    # Compile model
    model.compile(
        optimizer=Adam(learning_rate=learning_rate),
        loss="mse",
        metrics=["mae"]
    )
    
    return model

def objective(trial: optuna.Trial, X_train: np.ndarray, y_train: np.ndarray, 
             X_val: np.ndarray, y_val: np.ndarray) -> float:
    """
    Objective function for Optuna optimization.
    
    Args:
        trial: Optuna trial object
        X_train: Training features
        y_train: Training targets
        X_val: Validation features
        y_val: Validation targets
        
    Returns:
        Validation loss
    """
    # Create model
    model = create_rnn_model(trial, input_shape=(X_train.shape[1], X_train.shape[2]))
    
    # Define callbacks
    callbacks = [
        EarlyStopping(patience=10, restore_best_weights=True),
        TFKerasPruningCallback(trial, "val_loss")
    ]
    
    # Train model
    batch_size = trial.suggest_categorical("batch_size", [16, 32, 64, 128])
    history = model.fit(
        X_train, y_train,
        validation_data=(X_val, y_val),
        epochs=100,
        batch_size=batch_size,
        callbacks=callbacks,
        verbose=0
    )
    
    # Return best validation loss
    return min(history.history["val_loss"])

def train_rnn_models(rnn_data: Dict[str, Any], params: Dict[str, Any]) -> Dict[str, Any]:
    """
    Train RNN models for each frequency with Optuna optimization.
    
    Args:
        rnn_data: Dictionary containing data for RNN models
        params: Dictionary containing parameters
        
    Returns:
        Dictionary containing trained RNN models and predictions
    """
    logger.info("Training RNN models with Optuna optimization")
    
    result = {}
    
    for freq, data in rnn_data.items():
        logger.info(f"Training RNN model for {freq}")
        
        X_train = data["X_train"]
        y_train = data["y_train"]
        X_test = data["X_test"]
        y_test = data["y_test"]
        scaler = data["scaler"]
        original_y_test = data["original_y_test"]
        
        try:
            # Split training data into train and validation sets
            val_size = params.get("validation_size", 0.2)
            val_idx = int(len(X_train) * (1 - val_size))
            
            X_train_opt = X_train[:val_idx]
            y_train_opt = y_train[:val_idx]
            X_val = X_train[val_idx:]
            y_val = y_train[val_idx:]
            
            # Create Optuna study
            study = optuna.create_study(
                direction="minimize",
                pruner=optuna.pruners.MedianPruner(n_warmup_steps=10)
            )
            
            # Run optimization
            n_trials = params.get("n_trials", 20)
            logger.info(f"Starting hyperparameter optimization with {n_trials} trials")
            
            study.optimize(
                lambda trial: objective(trial, X_train_opt, y_train_opt, X_val, y_val),
                n_trials=n_trials
            )
            
            # Get best hyperparameters
            best_params = study.best_params
            logger.info(f"Best hyperparameters: {best_params}")
            
            # Create a trial with the best hyperparameters
            trial = optuna.trial.FixedTrial(best_params)
            
            # Create model with best hyperparameters
            model = create_rnn_model(trial, input_shape=(X_train.shape[1], X_train.shape[2]))
            
            # Define callbacks
            callbacks = [
                EarlyStopping(patience=15, restore_best_weights=True)
            ]
            
            # Train model
            batch_size = best_params["batch_size"]
            history = model.fit(
                X_train, y_train,
                validation_split=0.1,
                epochs=200,
                batch_size=batch_size,
                callbacks=callbacks,
                verbose=1
            )
            
            # Make predictions
            predictions_scaled = model.predict(X_test).flatten()
            
            # Inverse transform predictions
            if "y_scaler" in data:
                # For multivariate sequences with separate y scaler
                predictions = data["y_scaler"].inverse_transform(predictions_scaled.reshape(-1, 1)).flatten()
            else:
                # For univariate sequences
                predictions = scaler.inverse_transform(predictions_scaled.reshape(-1, 1)).flatten()
            
            # Calculate metrics
            mse = mean_squared_error(original_y_test, predictions)
            rmse = np.sqrt(mse)
            mae = mean_absolute_error(original_y_test, predictions)
            r2 = r2_score(original_y_test, predictions)
            
            logger.info(f"RNN {freq} - MSE: {mse:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f}, R²: {r2:.4f}")
            
            result[freq] = {
                "model": model,
                "predictions": predictions,
                "actual": original_y_test,
                "history": history.history,
                "best_params": best_params,
                "metrics": {
                    "mse": mse,
                    "rmse": rmse,
                    "mae": mae,
                    "r2": r2
                }
            }
        except Exception as e:
            logger.error(f"Error training RNN model for {freq}: {str(e)}")
            result[freq] = {
                "model": None,
                "predictions": None,
                "actual": original_y_test if 'original_y_test' in locals() else None,
                "metrics": {
                    "mse": float('inf'),
                    "rmse": float('inf'),
                    "mae": float('inf'),
                    "r2": float('-inf')
                },
                "error": str(e)
            }
    
    return result